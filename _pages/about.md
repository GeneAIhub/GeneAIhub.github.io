---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I'm a Ph.D. candidate in School of Electrical Engineering and Automation at Hefei University of Technology, and I'm now a visiting student in Faculty of Health Data Science at Juntendo University. I have published 5+ papers with 
 <a href='https://scholar.google.com/citations?user=WMkMTb4AAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>.

My research interest includes: 
- Physiological signal analysis
- Human-exoskeleton interaction
- Brain function connectivity analysis


# üéì Educations 
- *2019.09 - 2024.06*, Ph.D. in School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, China. <a href="https://en.hfut.edu.cn/"><img class="svg" src="/images/hfut.png" width="16pt"></a> 
- *2023.04 - 2024.04*, Ph.D. in Graduate School of Medicine, Juntendo University, Tokyo, Japan. (Visiting Student) <a href="https://en.juntendo.ac.jp/"><img class="svg" src="/images/juntendo.png" width="16pt"></a> 
- *2016.09 - 2019.06*, M.Sc. in School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, China. <a href="https://en.hfut.edu.cn/"><img class="svg" src="/images/hfut.png" width="16pt"></a> 
- *2012.09 - 2016.06*, B.Sc. in School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, China. <a href="https://en.hfut.edu.cn/"><img class="svg" src="/images/hfut.png" width="16pt"></a> 


# üìù Publications 
---
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ijaem2020.svg" alt="tag2text" width="190" height="110">
            </td>
            <td width="75%" valign="middle">
              <a href="https://recognize-anything.github.io/">
                <papertitle> Recognize Anything: A Strong Image Tagging Model </papertitle>
              </a>
              <br>
              Youcai Zhang*,
              <strong>Xinyu Huang*</strong>,
              Jinyu Ma*, Zhaoyang Li*, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, Yandong Guo, Lei Zhang
              <br>
              <em>Tech report</em>,
              2023
              <br>
              <a href="https://recognize-anything.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2306.03514">arXiv</a>
              /
              <a href="https://huggingface.co/spaces/xinyu1205/Tag2Text">demo</a>
              /
              <a href="https://github.com/xinyu1205/Recognize_Anything-Tag2Text">code</a>
              <p></p>
              <p><strong>Recognition and localization are two foundation computer vision tasks.</strong> <strong>The Segment Anything Model (SAM)</strong> excels in <strong>localization capabilities</strong>, while it falls short when it comes to <strong>recognition tasks</strong>. <strong>The Recognize Anything Model (RAM)</strong> exhibits <strong>exceptional recognition abilities</strong>, in terms of <strong>both accuracy and scope</strong>.</p>
            </td>
          </tr>
  </tbody>
</table>

---
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ijaem2020.svg" alt="tag2text" width="190" height="110">
            </td>
            <td width="75%" valign="middle">
              <a href="https://recognize-anything.github.io/">
                <papertitle> Recognize Anything: A Strong Image Tagging Model </papertitle>
              </a>
              <br>
              Youcai Zhang*,
              <strong>Xinyu Huang*</strong>,
              Jinyu Ma*, Zhaoyang Li*, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, Yandong Guo, Lei Zhang
              <br>
              <em>Tech report</em>,
              2023
              <br>
              <a href="https://recognize-anything.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2306.03514">arXiv</a>
              /
              <a href="https://huggingface.co/spaces/xinyu1205/Tag2Text">demo</a>
              /
              <a href="https://github.com/xinyu1205/Recognize_Anything-Tag2Text">code</a>
              <p></p>
              <p><strong>Recognition and localization are two foundation computer vision tasks.</strong> <strong>The Segment Anything Model (SAM)</strong> excels in <strong>localization capabilities</strong>, while it falls short when it comes to <strong>recognition tasks</strong>. <strong>The Recognize Anything Model (RAM)</strong> exhibits <strong>exceptional recognition abilities</strong>, in terms of <strong>both accuracy and scope</strong>.</p>
            </td>
          </tr>
  </tbody>
</table>


# üèÖ Honors and Awards
- *2022.10*  National Scholorship, China


# üí¨ News
- *2021.10*, None


# üè≠ Others
- None
  
